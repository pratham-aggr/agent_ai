"""
Synthetic Conversations Dataset Processor
========================================

Processes synthetic conversation datasets generated by the Hybrid Conversation Simulator.
Handles agent-customer dialogues with intent classification and confidence scoring.
"""

import logging
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from .utils import Dialogue, DialogueTurn, TextNormalizer, DataValidator

logger = logging.getLogger(__name__)

class SyntheticProcessor:
    """Processor for synthetic conversation datasets"""
    
    def __init__(self):
        self.normalizer = TextNormalizer()
        self.validator = DataValidator()
        
        # Synthetic conversation intent mapping
        self.intent_mapping = {
            'problem_report': 'report_issue',
            'acknowledge': 'acknowledge_issue',
            'gather_info': 'collect_information',
            'provide_info': 'provide_information',
            'solution_offer': 'offer_solution',
            'confirmation': 'confirm_details',
            'booking_request': 'request_booking',
            'payment_issue': 'payment_problem',
            'product_inquiry': 'product_question',
            'complaint': 'customer_complaint',
            'satisfaction': 'satisfaction_check',
            'escalation': 'escalate_issue',
            'resolution': 'resolve_issue',
            'follow_up': 'follow_up',
            'thank_you': 'thank_customer'
        }
    
    def process_dataset(self, csv_path: str) -> List[Dialogue]:
        """Process synthetic conversation dataset from CSV"""
        logger.info(f"Processing synthetic conversations from {csv_path}")
        
        try:
            # Load CSV data
            df = pd.read_csv(csv_path)
            logger.info(f"Loaded {len(df)} conversation turns")
            
            # Group by conversation_id
            conversations = df.groupby('conversation_id')
            dialogues = []
            
            for conv_id, conv_data in conversations:
                try:
                    dialogue = self._process_conversation(conv_id, conv_data)
                    if dialogue:
                        dialogues.append(dialogue)
                except Exception as e:
                    logger.warning(f"Failed to process conversation {conv_id}: {e}")
                    continue
            
            logger.info(f"Successfully processed {len(dialogues)} conversations")
            return dialogues
            
        except Exception as e:
            logger.error(f"Error loading synthetic dataset: {e}")
            return []
    
    def _process_conversation(self, conv_id: str, conv_data: pd.DataFrame) -> Optional[Dialogue]:
        """Process a single synthetic conversation"""
        try:
            # Sort by turn_id to ensure correct order
            conv_data = conv_data.sort_values('turn_id')
            
            # Extract conversation metadata
            intent_type = conv_data['intent_type'].iloc[0] if 'intent_type' in conv_data.columns else None
            model_used = conv_data['model_used'].iloc[0] if 'model_used' in conv_data.columns else None
            generation_timestamp = conv_data['generation_timestamp'].iloc[0] if 'generation_timestamp' in conv_data.columns else None
            
            # Process turns
            turns = []
            for _, row in conv_data.iterrows():
                turn = self._process_turn(row)
                if turn:
                    turns.append(turn)
            
            if not turns:
                return None
            
            # Create dialogue metadata
            metadata = {
                'intent_type': intent_type,
                'model_used': model_used,
                'generation_timestamp': generation_timestamp,
                'dataset': 'synthetic_conversations',
                'num_turns': len(turns),
                'success_rate': conv_data['success'].mean() if 'success' in conv_data.columns else None,
                'avg_confidence': conv_data['confidence'].mean() if 'confidence' in conv_data.columns else None
            }
            
            return Dialogue(
                dialogue_id=conv_id,
                turns=turns,
                domain=self._map_intent_to_domain(intent_type),
                intent_type=intent_type,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"Error processing conversation {conv_id}: {e}")
            return None
    
    def _process_turn(self, row: pd.Series) -> Optional[DialogueTurn]:
        """Process a single turn from synthetic data"""
        try:
            turn_id = int(row['turn_id'])
            speaker = row['speaker']
            utterance = row['utterance']
            
            # Normalize utterance
            utterance = self.normalizer.clean_utterance(utterance)
            if not utterance:
                return None
            
            # Extract intent
            intent = row.get('intent')
            if intent and intent in self.intent_mapping:
                intent = self.intent_mapping[intent]
            
            # Extract confidence
            confidence = row.get('confidence')
            if confidence is not None:
                confidence = float(confidence)
            
            # Extract entities (if available)
            entities = self._extract_entities(utterance, row)
            
            # Create turn metadata
            metadata = {
                'template_used': row.get('template_used'),
                'success': row.get('success'),
                'timestamp': row.get('timestamp'),
                'dataset_version': row.get('dataset_version')
            }
            
            return DialogueTurn(
                turn_id=turn_id,
                speaker=speaker.lower(),
                utterance=utterance,
                intent=intent,
                entities=entities,
                confidence=confidence,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"Error processing turn: {e}")
            return None
    
    def _extract_entities(self, utterance: str, row: pd.Series) -> Optional[Dict[str, str]]:
        """Extract entities from utterance and row data"""
        entities = {}
        
        # Extract entities from template if available
        template = row.get('template_used', '')
        if template:
            # Look for template variables like {product}, {problem_type}
            import re
            template_vars = re.findall(r'\{([^}]+)\}', template)
            
            # Try to extract values for these variables from utterance
            for var in template_vars:
                # Simple entity extraction based on common patterns
                if var == 'product':
                    # Look for product mentions
                    product_words = ['service', 'product', 'item', 'software', 'app', 'system']
                    for word in product_words:
                        if word in utterance.lower():
                            entities['product'] = word
                            break
                
                elif var == 'problem_type':
                    # Look for problem descriptions
                    problem_words = ['issue', 'problem', 'error', 'bug', 'malfunction', 'trouble']
                    for word in problem_words:
                        if word in utterance.lower():
                            entities['problem_type'] = word
                            break
                
                elif var == 'specific_details':
                    # Extract specific details
                    if len(utterance.split()) > 3:  # If utterance has enough detail
                        entities['specific_details'] = utterance[:100]  # First 100 chars
        
        # Extract common entities from utterance
        utterance_lower = utterance.lower()
        
        # Email addresses
        import re
        email_match = re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', utterance)
        if email_match:
            entities['email'] = email_match.group()
        
        # Phone numbers
        phone_match = re.search(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', utterance)
        if phone_match:
            entities['phone'] = phone_match.group()
        
        # Order numbers
        order_match = re.search(r'\b(?:order|order#|order number)[\s:]*([A-Z0-9-]+)\b', utterance_lower)
        if order_match:
            entities['order_number'] = order_match.group(1)
        
        # Amounts
        amount_match = re.search(r'\$(\d+(?:\.\d{2})?)', utterance)
        if amount_match:
            entities['amount'] = amount_match.group(1)
        
        return entities if entities else None
    
    def _map_intent_to_domain(self, intent_type: str) -> str:
        """Map intent type to domain"""
        domain_mapping = {
            'customer_support': 'customer_service',
            'booking_reservation': 'booking',
            'payment_billing': 'payment',
            'product_inquiry': 'product'
        }
        
        return domain_mapping.get(intent_type, 'general')
    
    def get_dataset_statistics(self, dialogues: List[Dialogue]) -> Dict[str, Any]:
        """Get statistics for processed synthetic dataset"""
        stats = self.validator.validate_dataset(dialogues)
        
        # Add synthetic-specific statistics
        intent_type_counts = {}
        model_counts = {}
        confidence_stats = {}
        success_stats = {}
        
        all_confidences = []
        all_successes = []
        
        for dialogue in dialogues:
            # Count intent types
            intent_type = dialogue.intent_type or 'unknown'
            intent_type_counts[intent_type] = intent_type_counts.get(intent_type, 0) + 1
            
            # Count models used
            model = dialogue.metadata.get('model_used', 'unknown')
            model_counts[model] = model_counts.get(model, 0) + 1
            
            # Collect confidence and success data
            for turn in dialogue.turns:
                if turn.confidence is not None:
                    all_confidences.append(turn.confidence)
                
                success = turn.metadata.get('success')
                if success is not None:
                    all_successes.append(success)
        
        # Calculate confidence statistics
        if all_confidences:
            confidence_stats = {
                'mean': sum(all_confidences) / len(all_confidences),
                'min': min(all_confidences),
                'max': max(all_confidences),
                'std': (sum((x - sum(all_confidences) / len(all_confidences))**2 for x in all_confidences) / len(all_confidences))**0.5
            }
        
        # Calculate success statistics
        if all_successes:
            success_rate = sum(all_successes) / len(all_successes)
            success_stats = {
                'success_rate': success_rate,
                'total_turns': len(all_successes),
                'successful_turns': sum(all_successes)
            }
        
        stats.update({
            'intent_type_distribution': intent_type_counts,
            'model_distribution': model_counts,
            'confidence_statistics': confidence_stats,
            'success_statistics': success_stats,
            'dataset_type': 'synthetic_conversations'
        })
        
        return stats
